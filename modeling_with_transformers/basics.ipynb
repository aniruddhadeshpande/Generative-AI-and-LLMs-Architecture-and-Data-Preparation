{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "243c0b3f",
   "metadata": {},
   "source": [
    "### üìò Learning Plan (simple ‚Üí deep ‚Üí full code)\n",
    "\n",
    "1. **Concept lesson** (short, clear explanation based on your uploaded guide)\n",
    "2. **Tiny PyTorch snippet** (to *see* how it works ‚Äî not full code yet)\n",
    "3. **One guiding question** (so you can confirm you understood)\n",
    "4. After all lessons ‚Üí **complete PyTorch code** for a small Transformer model with full explanation.\n",
    "\n",
    "We‚Äôll start slow so the final code feels natural, not scary.\n",
    "\n",
    "---\n",
    "\n",
    "# ‚≠ê Lesson 1: Word Embeddings\n",
    "\n",
    "(From your guide: converts words into numerical vectors that capture meaning )\n",
    "\n",
    "### üîπ Concept (simple)\n",
    "\n",
    "A Transformer cannot work with text directly, so each word is converted into a **vector** ‚Äî a list of numbers.\n",
    "Example from your guide:\n",
    "`\"cat\"` ‚Üí `[0.21, -0.45, 0.87, ‚Ä¶]`\n",
    "\n",
    "Why?\n",
    "Because numbers let the model measure similarity:\n",
    "\n",
    "* **cat** should be close to **dog**\n",
    "* **king** should be close to **queen**\n",
    "* **king ‚Äì man + woman ‚âà queen**\n",
    "\n",
    "These relationships form a **meaning space**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62fde90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3363,  0.7016,  0.2242, -0.5964, -0.2778],\n",
      "        [ 0.0348, -1.0133, -1.0489, -0.4451, -0.3193],\n",
      "        [-1.4741, -1.5419,  0.5382, -0.6469, -1.3666]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import token\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Suppose vocabulary has 10 words, each represented by a 5-dim vector\n",
    "embeddings = nn.Embedding(num_embeddings=10, embedding_dim=5)\n",
    "\n",
    "# A sentence represented as token ids [2,5,1]\n",
    "token = torch.tensor([2, 5, 1])\n",
    "\n",
    "# Convert token ids to embeddings\n",
    "vectors = embeddings(token)\n",
    "print(vectors) # Each row = meaning vector for a word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00079f73",
   "metadata": {},
   "source": [
    "Exactly right ‚Äî **a Transformer cannot understand raw text**, because it only works with **numbers**, so embeddings convert words into numeric meaning-vectors.\n",
    "Perfect, let‚Äôs move ahead.\n",
    "\n",
    "---\n",
    "\n",
    "# ‚≠ê Lesson 2: Positional Encoding\n",
    "\n",
    "(From your guide: transformers process all words at once, so they need positional signals to know order )\n",
    "\n",
    "### üîπ Concept (simple)\n",
    "\n",
    "Unlike RNNs, transformers do **not** read one word at a time.\n",
    "They look at the *entire* sentence simultaneously.\n",
    "\n",
    "So transformers need a way to know:\n",
    "\n",
    "* Which word is first?\n",
    "* Which word follows which?\n",
    "* What is the structure of the sentence?\n",
    "\n",
    "They solve this using **positional encoding**, which adds patterns based on sine/cosine waves to embeddings.\n",
    "\n",
    "It‚Äôs like saying:\n",
    "\n",
    "* ‚ÄúI‚Äôm the 1st word.‚Äù\n",
    "* ‚ÄúI‚Äôm the 2nd word.‚Äù\n",
    "* ‚ÄúI come after that one.‚Äù\n",
    "\n",
    "### Visual idea\n",
    "\n",
    "Imagine embedding = meaning.\n",
    "Positional encoding = location.\n",
    "Final vector = meaning + location.\n",
    "\n",
    "\n",
    "\n",
    "This prints a (5 √ó 6) matrix:\n",
    "5 positions, each with a 6-dim signal.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5e465b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000],\n",
      "        [ 0.8415,  0.5403,  0.0464,  0.9989,  0.0022,  1.0000],\n",
      "        [ 0.9093, -0.4161,  0.0927,  0.9957,  0.0043,  1.0000],\n",
      "        [ 0.1411, -0.9900,  0.1388,  0.9903,  0.0065,  1.0000],\n",
      "        [-0.7568, -0.6536,  0.1846,  0.9828,  0.0086,  1.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch #Loads PyTorch, which handles tensors and math operations.\n",
    "import math #Loads Python‚Äôs math functions (we don‚Äôt directly use it here but it's common in formulas).\n",
    "\n",
    "def positional_encoding(sequence_len, d_model): #sequence_len = number of tokens (words), d_model = size of embedding vector, example: 5 words, each represented by a 6-dim vector ‚Üí (5, 6) positional encoding.\n",
    "    pos = torch.arange(sequence_len).unsqueeze(1)          # shape: (seq, 1)\n",
    "    i = torch.arange(d_model).unsqueeze(0)                 # shape: (1, d_model)\n",
    "\n",
    "    angles = pos / (10000 ** (2 * (i//2) / d_model))       # formula from paper\n",
    "\n",
    "    # apply sin to even dims, cos to odd dims\n",
    "    pe = torch.zeros(sequence_len, d_model)\n",
    "    pe[:, 0::2] = torch.sin(angles[:, 0::2])\n",
    "    pe[:, 1::2] = torch.cos(angles[:, 1::2])\n",
    "\n",
    "    return pe\n",
    "\n",
    "pe = positional_encoding(5, 6)\n",
    "print(pe)\n",
    "#This prints a (5 √ó 6) matrix:\n",
    "#5 positions, each with a 6-dim signal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88d6f49",
   "metadata": {},
   "source": [
    "# ‚≠ê Line-by-line Explanation\n",
    "\n",
    "### `import torch`\n",
    "\n",
    "Loads **PyTorch**, which handles tensors and math operations.\n",
    "\n",
    "### `import math`\n",
    "\n",
    "Loads Python‚Äôs math functions (we don‚Äôt directly use it here but it's common in formulas).\n",
    "\n",
    "---\n",
    "\n",
    "# ‚≠ê Inside the function\n",
    "\n",
    "### `def positional_encoding(sequence_len, d_model):`\n",
    "\n",
    "Defines a function:\n",
    "\n",
    "* `sequence_len` = number of tokens (words)\n",
    "* `d_model` = size of embedding vector\n",
    "\n",
    "Example:\n",
    "5 words, each represented by a 6-dim vector ‚Üí `(5, 6)` positional encoding.\n",
    "\n",
    "---\n",
    "\n",
    "### `pos = torch.arange(sequence_len).unsqueeze(1)`\n",
    "\n",
    "Breakdown:\n",
    "\n",
    "1. `torch.arange(sequence_len)`\n",
    "   Creates: `[0, 1, 2, 3, 4]` (for 5 positions)\n",
    "2. `.unsqueeze(1)` adds one extra dimension ‚Üí shape becomes `(seq_len, 1)`\n",
    "\n",
    "So:\n",
    "\n",
    "```\n",
    "[[0],\n",
    " [1],\n",
    " [2],\n",
    " [3],\n",
    " [4]]\n",
    "```\n",
    "\n",
    "This is the **position number** for each word.\n",
    "\n",
    "---\n",
    "\n",
    "### `i = torch.arange(d_model).unsqueeze(0)`\n",
    "\n",
    "Breakdown:\n",
    "\n",
    "1. `torch.arange(d_model)`\n",
    "   If `d_model=6` ‚Üí `[0, 1, 2, 3, 4, 5]`\n",
    "2. `.unsqueeze(0)` ‚Üí shape `(1, d_model)`\n",
    "\n",
    "So:\n",
    "\n",
    "```\n",
    "[[0, 1, 2, 3, 4, 5]]\n",
    "```\n",
    "\n",
    "`i` represents **each dimension in the embedding**.\n",
    "\n",
    "---\n",
    "\n",
    "### `angles = pos / (10000 ** (2 * (i//2) / d_model))`\n",
    "\n",
    "This is the official Transformer sinusoidal formula.\n",
    "\n",
    "Breakdown:\n",
    "\n",
    "* `i // 2` makes pairs ‚Üí `[0, 0, 1, 1, 2, 2]`\n",
    "* `(2 * (i//2) / d_model)` determines frequencies\n",
    "* `10000 ** (...)` makes big frequency denominators\n",
    "* `pos / (...)` applies the formula for each position\n",
    "\n",
    "Result: a `(seq_len √ó d_model)` matrix of raw angle values.\n",
    "\n",
    "This matrix will later get sin and cos applied.\n",
    "\n",
    "---\n",
    "\n",
    "### `pe = torch.zeros(sequence_len, d_model)`\n",
    "\n",
    "Creates an empty positional encoding matrix of shape `(seq_len, d_model)`\n",
    "\n",
    "---\n",
    "\n",
    "### `pe[:, 0::2] = torch.sin(angles[:, 0::2])`\n",
    "\n",
    "* `0::2` picks **even columns** (0, 2, 4, ‚Ä¶)\n",
    "* Applies **sin** to even dimensions\n",
    "\n",
    "This matches the original Transformer design:\n",
    "\n",
    "* Even dims ‚Üí sine wave\n",
    "\n",
    "---\n",
    "\n",
    "### `pe[:, 1::2] = torch.cos(angles[:, 1::2])`\n",
    "\n",
    "* `1::2` picks **odd columns** (1, 3, 5, ‚Ä¶)\n",
    "* Applies **cos** to odd dims\n",
    "\n",
    "Odd dims ‚Üí cosine wave.\n",
    "\n",
    "---\n",
    "\n",
    "### `return pe`\n",
    "\n",
    "Returns the complete positional encoding matrix.\n",
    "\n",
    "---\n",
    "\n",
    "### `pe = positional_encoding(5, 6)`\n",
    "\n",
    "Build positional encoding for:\n",
    "\n",
    "* sequence of length 5\n",
    "* embedding size 6\n",
    "\n",
    "---\n",
    "\n",
    "### `print(pe)`\n",
    "\n",
    "Prints the final matrix.\n",
    "\n",
    "---\n",
    "\n",
    "# ‚≠ê Quick Check (your turn)\n",
    "\n",
    "Just one question (as per study rule):\n",
    "\n",
    "**Why does the code use sine for even dimensions and cosine for odd dimensions?**\n",
    "Answer in one simple sentence.\n",
    "Answer-> Transformers use sine (even dims) and cosine (odd dims) so each dimension carries a different frequency pattern, making every position uniquely identifiable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f236bb4",
   "metadata": {},
   "source": [
    "Good! You‚Äôre on the right track ‚Äî **the sin/cos pattern creates different frequencies**, which helps the model distinguish positions.\n",
    "Let me refine it slightly so the idea becomes crystal clear:\n",
    "\n",
    "### ‚úî Corrected idea\n",
    "\n",
    "**Transformers use sine (even dims) and cosine (odd dims) so each dimension carries a different frequency pattern, making every position uniquely identifiable.**\n",
    "\n",
    "Perfect ‚Äî now you‚Äôre ready for the most important part of Transformers.\n",
    "\n",
    "---\n",
    "\n",
    "# ‚≠ê Lesson 3: Self-Attention\n",
    "\n",
    "(Based fully on your uploaded guide‚Äôs explanation of Q, K, V and scaled dot-product attention )\n",
    "\n",
    "### üîπ Concept (simple)\n",
    "\n",
    "Self-attention answers one question:\n",
    "\n",
    "**‚ÄúWhen the model reads a word, which other words should it pay attention to?‚Äù**\n",
    "\n",
    "Example:\n",
    "In the sentence:\n",
    "**‚ÄúThe cat sat on the mat.‚Äù**\n",
    "When the model reads **‚Äúcat‚Äù**, it should pay attention to **‚Äúsat‚Äù**, not **‚Äúmat.‚Äù**\n",
    "\n",
    "Self-attention does this by computing three vectors for each word:\n",
    "\n",
    "* **Q (Query)** ‚Äî what is this word looking for?\n",
    "* **K (Key)** ‚Äî what information does this word contain?\n",
    "* **V (Value)** ‚Äî what content to pass to the next layer?\n",
    "\n",
    "Then compute:\n",
    "\n",
    "```\n",
    "Attention = softmax( Q ¬∑ K·µÄ / sqrt(d) ) ¬∑ V\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1447d63f",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Is this `softmax` the same as the one used in losses?\n",
    "\n",
    "Yes ‚Äî **it‚Äôs the same mathematical softmax** (converts a vector of real numbers into a probability distribution that sums to 1).\n",
    "\n",
    "But the *purpose* differs by context:\n",
    "\n",
    "* **In attention:** `softmax` is applied to the attention *scores* (Q¬∑K·µÄ / ‚àöd) to produce **attention weights** ‚Äî i.e., how much each token should contribute when computing a token‚Äôs new representation.\n",
    "* **In classification/loss:** `softmax` turns model logits into class probabilities; those probabilities are then compared with labels (via cross-entropy) to compute loss.\n",
    "\n",
    "So same function, different semantics: in attention it yields *which tokens to focus on*; in classification it yields *probability of each class*.\n",
    "\n",
    "---\n",
    "\n",
    "### Extra intuition notes (2 short points)\n",
    "\n",
    "* **Why scale by ‚àöd?**\n",
    "  Dot products grow with `d`; dividing by ‚àöd keeps the softmax input magnitudes reasonable so gradients don‚Äôt vanish/explode.\n",
    "* **Multi-head attention:**\n",
    "  In practice we split `d_model` into several heads (e.g., 8 heads). Each head computes attention with a smaller dimension and then we concatenate heads ‚Äî this helps the model capture different types of relationships.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9a6607d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention output:\n",
      " tensor([[-0.2467,  0.0866,  0.6043,  0.5489],\n",
      "        [-0.0777,  0.2542,  0.5946,  0.7298],\n",
      "        [-0.4481, -0.0998,  0.6194,  0.3523]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "d_model = 4   # small dimension\n",
    "x = torch.randn(3, d_model)  # 3 tokens, each a d_model-dimensional vector\n",
    "\n",
    "W_Q = nn.Linear(d_model, d_model)\n",
    "W_K = nn.Linear(d_model, d_model)\n",
    "W_V = nn.Linear(d_model, d_model)\n",
    "\n",
    "Q = W_Q(x)\n",
    "K = W_K(x)\n",
    "V = W_V(x)\n",
    "\n",
    "scores = Q @ K.transpose(0, 1) / torch.sqrt(torch.tensor(d_model, dtype=torch.float32))\n",
    "attention_weights = torch.softmax(scores, dim=-1)\n",
    "output = attention_weights @ V\n",
    "\n",
    "print(\"Attention output:\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2488d2",
   "metadata": {},
   "source": [
    "## line-by-line explanation:\n",
    "\n",
    "* `import torch`\n",
    "  Loads PyTorch (tensor operations, GPU support, etc).\n",
    "\n",
    "* `import torch.nn as nn`\n",
    "  Imports PyTorch neural-network helpers (layers like `Linear`).\n",
    "\n",
    "* `d_model = 4`\n",
    "  Sets the embedding/hidden size to 4 here (toy example). In real models `d_model` might be 256, 512, 768, etc.\n",
    "\n",
    "* `x = torch.randn(3, d_model)`\n",
    "  Creates a random tensor shaped `(3, 4)` ‚Äî **3 tokens** (sequence length = 3), each token represented by a 4-dim vector. Think of `x` as the input embeddings to the attention layer.\n",
    "\n",
    "* `W_Q = nn.Linear(d_model, d_model)`\n",
    "  `W_Q` is a learnable linear layer that maps an input vector to a **Query** vector of size `d_model`. Internally it contains weights and bias.\n",
    "\n",
    "* `W_K = nn.Linear(d_model, d_model)`\n",
    "  Similar but for **Key** vectors.\n",
    "\n",
    "* `W_V = nn.Linear(d_model, d_model)`\n",
    "  Similar but for **Value** vectors.\n",
    "\n",
    "  *(In full implementations these are often a single `nn.Linear` producing concatenated Q/K/V or implemented as `nn.Linear(d_model, 3*d_model)` for efficiency.)*\n",
    "\n",
    "* `Q = W_Q(x)`\n",
    "  Applies the `W_Q` linear transform to every token embedding. Result `Q` has shape `(3, d_model)` and is the set of Query vectors for each token.\n",
    "\n",
    "* `K = W_K(x)`\n",
    "  Applies `W_K` to get Key vectors; shape `(3, d_model)`.\n",
    "\n",
    "* `V = W_V(x)`\n",
    "  Applies `W_V` to get Value vectors; shape `(3, d_model)`.\n",
    "\n",
    "* `scores = Q @ K.transpose(0, 1) / torch.sqrt(torch.tensor(d_model, dtype=torch.float32))`\n",
    "  This is the **scaled dot-product** step:\n",
    "\n",
    "  * `Q @ K.transpose(0, 1)` computes dot products between every Query and every Key. If `Q` is `(3, d)` and `K·µÄ` is `(d, 3)` the result is `(3, 3)`: for each of the 3 queries we have a score for each of the 3 keys.\n",
    "  * ` / sqrt(d_model)` scales down the scores by ‚àöd (stabilizes gradients and prevents extremely large softmax inputs when `d` is big).\n",
    "  * So `scores[i, j]` = similarity of token i‚Äôs Query with token j‚Äôs Key (higher ‚Üí more attention).\n",
    "\n",
    "* `attention_weights = torch.softmax(scores, dim=-1)`\n",
    "  Applies `softmax` row-wise (across keys for each query). Each row becomes a probability distribution over the tokens the query can attend to.\n",
    "  Example: `attention_weights[0]` might be `[0.1, 0.8, 0.1]` meaning query 0 mostly attends to token 1.\n",
    "\n",
    "* `output = attention_weights @ V`\n",
    "  Weighted sum of the Value vectors: for each query (each row), we multiply attention weights with the corresponding `V` rows and sum ‚Üí yields new representation for each token.\n",
    "  If `attention_weights` is `(3,3)` and `V` is `(3,d)`, `output` is `(3,d)`.\n",
    "\n",
    "* `print(\"Attention output:\\n\", output)`\n",
    "  Prints the final tensor: the attended representations for each token.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba72d907",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ‚≠ê LESSON 4 ‚Äî Transformer Encoder & Decoder\n",
    "\n",
    "(from your PDF ‚Äî embeddings ‚Üí positional encoding ‚Üí encoder/decoder ‚Üí linear layer)\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£ What the Encoder Does\n",
    "\n",
    "Your PDF says:\n",
    "**‚ÄúEncoder uses unmasked self-attention (bi-directional)‚Äù** \n",
    "\n",
    "### üîπ Simple meaning\n",
    "\n",
    "The **encoder reads the entire input sentence** and understands relationships between ALL words.\n",
    "\n",
    "Examples:\n",
    "\n",
    "* Input: ‚ÄúThe cat sat on the mat‚Äù\n",
    "  Encoder figures out relationships like:\n",
    "\n",
    "  * ‚Äúcat‚Äù ‚Üî ‚Äúsat‚Äù\n",
    "  * ‚Äúmat‚Äù ‚Üî ‚Äúon‚Äù\n",
    "\n",
    "### üîπ Why ‚Äúunmasked‚Äù?\n",
    "\n",
    "Encoder is allowed to **look at all words** ‚Äî past + future ‚Äî because it's only *understanding*, not predicting.\n",
    "\n",
    "---\n",
    "\n",
    "# 2Ô∏è‚É£ What the Decoder Does\n",
    "\n",
    "Your PDF says:\n",
    "**‚ÄúDecoder predicts the next word‚Ä¶ uses **masked** multi-head self-attention to avoid looking ahead‚Äù** \n",
    "\n",
    "### üîπ Simple meaning\n",
    "\n",
    "Decoder is the part that **generates text**, one word at a time.\n",
    "\n",
    "To stay realistic, decoder must NOT cheat.\n",
    "So it sees:\n",
    "\n",
    "* previous words ‚úî\n",
    "* current position ‚úî\n",
    "* future words ‚ùå (mask hides them)\n",
    "\n",
    "Example (predicting word 3):\n",
    "\n",
    "* Words available: token 1, token 2\n",
    "* Words NOT available: token 4, token 5\n",
    "\n",
    "---\n",
    "\n",
    "# 3Ô∏è‚É£ Encoder‚ÄìDecoder Attention (the bridge)\n",
    "\n",
    "Your PDF describes:\n",
    "**‚ÄúDecoder uses encoder output to generate final output‚Äù** \n",
    "\n",
    "### üîπ Meaning\n",
    "\n",
    "Decoder asks the encoder:\n",
    "\n",
    "> ‚ÄúHey encoder, which parts of the input are relevant for the word I'm generating?‚Äù\n",
    "\n",
    "Example:\n",
    "In English ‚Üí French translation:\n",
    "\n",
    "* Decoder generating *‚Äúmange‚Äù*\n",
    "  attends to **‚Äúeat‚Äù** in the encoder output.\n",
    "\n",
    "---\n",
    "\n",
    "# 4Ô∏è‚É£ Putting It All Together (simple flow)\n",
    "\n",
    "We now combine everything you've learned:\n",
    "\n",
    "**Input text**\n",
    "‚Üí Embedding\n",
    "‚Üí Positional Encoding\n",
    "‚Üí **Encoder (full self-attention)**\n",
    "‚Üí **Decoder (masked self-attention + encoder-decoder attention)**\n",
    "‚Üí Linear Layer\n",
    "‚Üí Softmax ‚Üí next word\n",
    "\n",
    "That‚Äôs the entire Transformer loop.\n",
    "\n",
    "---\n",
    "\n",
    "# ‚≠ê ONE GUIDING QUESTION (your turn)\n",
    "\n",
    "**Why must the decoder use ‚Äúmasked‚Äù self-attention, but the encoder does not?**\n",
    "Answer in your own words, one sentence.\n",
    "\n",
    "Answer: The decoder must be masked because it generates the next word and should not see future words, while the encoder is only understanding the input and can see all words.\n",
    "\n",
    "Once you answer, I‚Äôll confirm and then teach **Multi-Head Attention**, followed by **complete PyTorch Transformer code** (your request).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
